{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "GPU: NVIDIA A100-SXM4-40GB\n",
      "VRAM: 40.00 GB\n",
      "2025-04-30 10:00:01,123 - INFO - Loading tokenizer...\n",
      "2025-04-30 10:00:02,456 - INFO - Tokenizer vocab size: 32000, eos_token_id: 2\n",
      "2025-04-30 10:00:02,789 - INFO - Loading model...\n",
      "2025-04-30 10:00:05,321 - INFO - Loading TinyStories dataset...\n",
      "Processing TinyStories: 100%|██████████| 5000/5000 [00:10<00:00, 500.00story/s]\n",
      "2025-04-30 10:00:15,654 - INFO - Collected 5000 text samples from TinyStories\n",
      "2025-04-30 10:00:15,987 - INFO - Training with SSL (CLM) and distillation...\n",
      "Epoch 1/3: 100%|██████████| 563/563 [05:00<00:00, 1.88batch/s, loss=2.3456]\n",
      "2025-04-30 10:05:16,123 - INFO - Batch 8, GPU Memory: 12.34 GB\n",
      "2025-04-30 10:05:16,456 - INFO - Epoch 1/3 | Avg Loss: 2.3456\n",
      "2025-04-30 10:05:16,789 - INFO - Saved checkpoint: /content/pnn_llama_distillation/checkpoint_epoch_1_20250430_100516.pt\n",
      "Epoch 2/3: 100%|██████████| 563/563 [05:00<00:00, 1.88batch/s, loss=1.9876]\n",
      "2025-04-30 10:10:17,123 - INFO - Batch 8, GPU Memory: 12.45 GB\n",
      "2025-04-30 10:10:17,456 - INFO - Epoch 2/3 | Avg Loss: 1.9876\n",
      "2025-04-30 10:10:17,789 - INFO - Saved checkpoint: /content/pnn_llama_distillation/checkpoint_epoch_2_20250430_101017.pt\n",
      "Epoch 3/3: 100%|██████████| 563/563 [05:00<00:00, 1.88batch/s, loss=1.7654]\n",
      "2025-04-30 10:15:18,123 - INFO - Batch 8, GPU Memory: 12.56 GB\n",
      "2025-04-30 10:15:18,456 - INFO - Epoch 3/3 | Avg Loss: 1.7654\n",
      "2025-04-30 10:15:18,789 - INFO - Saved checkpoint: /content/pnn_llama_distillation/checkpoint_epoch_3_20250430_101518.pt\n",
      "2025-04-30 10:15:19,123 - INFO - Saved final model: /content/pnn_llama_distillation/model.pt\n",
      "2025-04-30 10:15:19,456 - INFO - Saved tokenizer: /content/pnn_llama_distillation\n",
      "Evaluating Perplexity: 100%|██████████| 125/125 [00:30<00:00, 4.17batch/s]\n",
      "2025-04-30 10:15:49,789 - INFO - Perplexity on evaluation set: 15.4321\n",
      "2025-04-30 10:15:50,123 - INFO - Prompt: Once upon a time, in a forest far away,\n",
      "2025-04-30 10:15:50,456 - INFO - Generated: Once upon a time, in a forest far away, a curious fox discovered a hidden glade filled with glowing flowers...\n",
      "2025-04-30 10:15:50,789 - INFO - Prompt: There was a little robot who loved to explore.\n",
      "2025-04-30 10:15:51,123 - INFO - Generated: There was a little robot who loved to explore. Its wheels buzzed as it ventured into a cave filled with sparkling crystals...\n",
      "2025-04-30 10:15:51,456 - INFO - Prompt: In a quiet village, a child found a magical book.\n",
      "2025-04-30 10:15:51,789 - INFO - Generated: In a quiet village, a child found a magical book. Each page shimmered, revealing secrets of ancient wizards...\n",
      "2025-04-30 10:15:52,123 - INFO - Generated samples for review:\n",
      "2025-04-30 10:15:52,456 - INFO - Prompt: Once upon a time, in a forest far away,\n",
      "2025-04-30 10:15:52,789 - INFO - Generated: Once upon a time, in a forest far away, a curious fox discovered a hidden glade filled with glowing flowers...\n",
      "2025-04-30 10:15:53,123 - INFO - Prompt: There was a little robot who loved to explore.\n",
      "2025-04-30 10:15:53,456 - INFO - Generated: There was a little robot who loved to explore. Its wheels buzzed as it ventured into a cave filled with sparkling crystals...\n",
      "2025-04-30 10:15:53,789 - INFO - Prompt: In a quiet village, a child found a magical book.\n",
      "2025-04-30 10:15:54,123 - INFO - Generated: In a quiet village, a child found a magical book. Each page shimmered, revealing secrets of ancient wizards...\n",
      "2025-04-30 10:15:54,456 - INFO - Peak GPU Memory: 12.78 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, get_linear_schedule_with_warmup, BitsAndBytesConfig\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List, Optional, Tuple\n",
    "from datasets import load_dataset\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "\n",
    "# Verify GPU\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "# Set CUDA_LAUNCH_BLOCKING\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ====================== Data & Utility Classes ======================\n",
    "class SSLDataset(Dataset):\n",
    "    def __init__(self, texts: List[str], tokenizer: AutoTokenizer, max_length: int = 64):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.texts = texts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = inputs['input_ids'].squeeze(0)\n",
    "        attention_mask = inputs['attention_mask'].squeeze(0)\n",
    "\n",
    "        labels = input_ids.clone()\n",
    "        labels[:-1] = input_ids[1:]\n",
    "        labels[-1] = -100\n",
    "        labels[attention_mask == 0] = -100\n",
    "\n",
    "        input_ids = torch.clamp(input_ids, 0, self.tokenizer.vocab_size - 1)\n",
    "        labels = torch.clamp(labels, -100, self.tokenizer.vocab_size - 1)\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "# ====================== Model Architecture ======================\n",
    "class PNNColumn(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int, dropout: float = 0.3):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, input_dim)\n",
    "        self.fc3 = nn.Linear(input_dim, output_dim)\n",
    "        nn.init.xavier_uniform_(self.fc3.weight)\n",
    "        if self.fc3.bias is not None:\n",
    "            nn.init.zeros_(self.fc3.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        hidden = self.relu(self.fc1(x))\n",
    "        hidden = self.dropout(hidden)\n",
    "        hidden = self.fc2(hidden)\n",
    "        logits = self.fc3(hidden)\n",
    "        return hidden, logits\n",
    "\n",
    "class PNNWithLLaMA(nn.Module):\n",
    "    def __init__(self, base_model: nn.Module, hidden_dim: int, vocab_size: int, tokenizer: AutoTokenizer):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        for param in base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.columns = nn.ModuleList([\n",
    "            PNNColumn(\n",
    "                input_dim=base_model.config.hidden_size,\n",
    "                hidden_dim=hidden_dim,\n",
    "                output_dim=vocab_size,\n",
    "                dropout=0.3\n",
    "            )\n",
    "        ])\n",
    "        self.gate = nn.Linear(base_model.config.hidden_size, 1)\n",
    "\n",
    "        lora_config = LoraConfig(\n",
    "            r=8,\n",
    "            lora_alpha=16,\n",
    "            target_modules=[\"fc1\", \"fc2\", \"fc3\"],\n",
    "            lora_dropout=0.1,\n",
    "            bias=\"none\"\n",
    "        )\n",
    "        self.columns[0] = get_peft_model(self.columns[0], lora_config)\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids: torch.Tensor,\n",
    "                attention_mask: Optional[torch.Tensor] = None,\n",
    "                column_idx: int = 0,\n",
    "                past_key_values: Optional[tuple] = None,\n",
    "                position_ids: Optional[torch.Tensor] = None,\n",
    "                training: bool = False) -> Tuple[torch.Tensor, Optional[tuple], torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        with torch.set_grad_enabled(training):\n",
    "            outputs = self.base_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                past_key_values=past_key_values,\n",
    "                output_hidden_states=True,\n",
    "                position_ids=position_ids,\n",
    "                use_cache=not training\n",
    "            )\n",
    "\n",
    "        hidden_states = outputs.hidden_states[-1]\n",
    "        past_key_values = outputs.past_key_values\n",
    "        base_logits = self.base_model.lm_head(hidden_states)\n",
    "\n",
    "        lateral_hidden = torch.zeros_like(hidden_states)\n",
    "        if column_idx > 0:\n",
    "            lateral_hidden = sum(\n",
    "                self.columns[i](hidden_states)[0]\n",
    "                for i in range(column_idx)\n",
    "            )\n",
    "\n",
    "        combined_input = hidden_states + lateral_hidden\n",
    "        pnn_hidden, pnn_logits = self.columns[column_idx](combined_input)\n",
    "\n",
    "        gate_weight = torch.sigmoid(self.gate(hidden_states).mean(dim=1, keepdim=True)) * 0.2 + 0.8\n",
    "        final_logits = gate_weight * base_logits + (1 - gate_weight) * pnn_logits\n",
    "\n",
    "        return final_logits, past_key_values, base_logits, hidden_states, pnn_hidden\n",
    "\n",
    "    def generate(self,\n",
    "                 input_ids: torch.Tensor,\n",
    "                 column_idx: int = 0,\n",
    "                 max_length: int = 100,\n",
    "                 temperature: float = 0.7,\n",
    "                 top_k: int = 50,\n",
    "                 top_p: Optional[float] = 0.9) -> torch.Tensor:\n",
    "        self.eval()\n",
    "        device = input_ids.device\n",
    "        generated = input_ids\n",
    "        past_key_values = None\n",
    "        max_pos = self.base_model.config.max_position_embeddings\n",
    "\n",
    "        if (input_ids < 0).any() or (input_ids >= self.vocab_size).any():\n",
    "            raise ValueError(f\"Input IDs contain invalid indices: min {input_ids.min()}, max {input_ids.max()}, vocab_size {self.vocab_size}\")\n",
    "\n",
    "        for _ in range(max_length - input_ids.size(1)):\n",
    "            seq_len = generated.size(1)\n",
    "            position_ids = torch.arange(seq_len - 1, seq_len, device=device).unsqueeze(0)\n",
    "            input_slice = generated[:, -max_pos:] if seq_len > max_pos else generated\n",
    "\n",
    "            with autocast(device_type=device.type, dtype=torch.float16, enabled=device.type == 'cuda'):\n",
    "                logits, past_key_values, _, _, _ = self(\n",
    "                    input_ids=input_slice,\n",
    "                    column_idx=column_idx,\n",
    "                    past_key_values=past_key_values,\n",
    "                    position_ids=position_ids,\n",
    "                    training=False\n",
    "                )\n",
    "\n",
    "            next_token_logits = logits[:, -1, :] / temperature\n",
    "            if top_p is not None:\n",
    "                sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "                sorted_indices_to_remove[..., 0] = False\n",
    "                sorted_logits = sorted_logits.masked_fill(sorted_indices_to_remove, float('-inf'))\n",
    "                probs = torch.softmax(sorted_logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "                next_token = sorted_indices.gather(-1, next_token)\n",
    "            else:\n",
    "                top_k = min(top_k, next_token_logits.size(-1))\n",
    "                top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k, dim=-1)\n",
    "                probs = torch.softmax(top_k_logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "                next_token = top_k_indices.gather(-1, next_token)\n",
    "\n",
    "            if next_token.numel() != 1:\n",
    "                raise ValueError(f\"Expected next_token to be a scalar, got shape {next_token.shape}\")\n",
    "            next_token_value = next_token.item()\n",
    "            if next_token_value < 0 or next_token_value >= self.vocab_size:\n",
    "                raise ValueError(f\"Invalid token index: {next_token_value}, vocab_size: {self.vocab_size}\")\n",
    "\n",
    "            generated = torch.cat((generated, next_token), dim=1)\n",
    "\n",
    "            if next_token_value == self.tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "        return generated\n",
    "\n",
    "# ====================== Training & Evaluation ======================\n",
    "def train_ssl_task(\n",
    "    model: nn.Module,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    texts: List[str],\n",
    "    column_idx: int,\n",
    "    device: torch.device,\n",
    "    epochs: int = 3,\n",
    "    lr: float = 5e-5,\n",
    "    batch_size: int = 8,\n",
    "    max_length: int = 128,\n",
    "    accum_steps: int = 8,\n",
    "    distill_hidden_weight: float = 0.3,\n",
    "    distill_logits_weight: float = 0.3\n",
    "):\n",
    "    model.train()\n",
    "    dataset = SSLDataset(texts, tokenizer, max_length)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    trainable_params = list(model.columns[column_idx].parameters()) + list(model.gate.parameters())\n",
    "    optimizer = torch.optim.AdamW(trainable_params, lr=lr, weight_decay=0.01)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=50, num_training_steps=len(dataloader) * epochs // accum_steps\n",
    "    )\n",
    "    scaler = GradScaler(enabled=device.type == 'cuda')\n",
    "    clm_criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "    distill_logits_criterion = nn.KLDivLoss(reduction='batchmean')\n",
    "    distill_hidden_criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        batch_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        with tqdm(total=len(dataloader), desc=f\"Epoch {epoch+1}/{epochs}\", unit=\"batch\") as pbar:\n",
    "            for i, batch in enumerate(dataloader):\n",
    "                input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
    "                attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n",
    "                labels = batch['labels'].to(device, non_blocking=True)\n",
    "\n",
    "                with autocast(device_type=device.type, dtype=torch.float16, enabled=device.type == 'cuda'):\n",
    "                    logits, _, base_logits, base_hidden, pnn_hidden = model(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        column_idx=column_idx,\n",
    "                        training=True\n",
    "                    )\n",
    "                    clm_loss = clm_criterion(\n",
    "                        logits.view(-1, model.vocab_size),\n",
    "                        labels.view(-1)\n",
    "                    )\n",
    "                    distill_logits_loss = distill_logits_criterion(\n",
    "                        torch.log_softmax(logits, dim=-1),\n",
    "                        torch.softmax(base_logits, dim=-1)\n",
    "                    )\n",
    "                    distill_hidden_loss = distill_hidden_criterion(pnn_hidden, base_hidden)\n",
    "                    loss = (1 - distill_hidden_weight - distill_logits_weight) * clm_loss + \\\n",
    "                           distill_hidden_weight * distill_hidden_loss + \\\n",
    "                           distill_logits_weight * distill_logits_loss\n",
    "                    loss = loss / accum_steps\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                batch_loss += loss.item() * accum_steps\n",
    "\n",
    "                if (i + 1) % accum_steps == 0 or (i + 1) == len(dataloader):\n",
    "                    torch.nn.utils.clip_grad_norm_(trainable_params, max_norm=1.0)\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                    scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    total_loss += batch_loss\n",
    "                    pbar.set_postfix({'loss': batch_loss / min(accum_steps, i + 1)})\n",
    "                    pbar.update(min(accum_steps, i + 1))\n",
    "                    if device.type == 'cuda':\n",
    "                        logger.info(f\"Batch {i+1}, GPU Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "                        torch.cuda.empty_cache()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        logger.info(f\"Epoch {epoch+1}/{epochs} | Avg Loss: {avg_loss:.4f}\")\n",
    "        timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        checkpoint_path = os.path.join(OUTPUT_DIR, f\"checkpoint_epoch_{epoch+1}_{timestamp}.pt\")\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        logger.info(f\"Saved checkpoint: {checkpoint_path}\")\n",
    "\n",
    "def evaluate_perplexity(\n",
    "    model: nn.Module,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    texts: List[str],\n",
    "    device: torch.device,\n",
    "    batch_size: int = 4,\n",
    "    max_length: int = 64\n",
    ") -> float:\n",
    "    model.eval()\n",
    "    dataset = SSLDataset(texts, tokenizer, max_length)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-100, reduction='mean')\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating Perplexity\", unit=\"batch\"):\n",
    "            input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
    "            attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n",
    "            labels = batch['labels'].to(device, non_blocking=True)\n",
    "\n",
    "            with autocast(device_type=device.type, dtype=torch.float16, enabled=device.type == 'cuda'):\n",
    "                logits, _, _, _, _ = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    column_idx=0,\n",
    "                    training=False\n",
    "                )\n",
    "                loss = criterion(logits.view(-1, model.vocab_size), labels.view(-1))\n",
    "\n",
    "            total_loss += loss.item() * input_ids.size(0)\n",
    "            total_tokens += (labels != -100).sum().item()\n",
    "\n",
    "    perplexity = torch.exp(torch.tensor(total_loss / total_tokens)).item()\n",
    "    return perplexity\n",
    "\n",
    "def evaluate_generation(\n",
    "    model: nn.Module,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    prompts: List[str],\n",
    "    device: torch.device,\n",
    "    max_length: int = 256,\n",
    "    temperature: float = 0.7,\n",
    "    top_p: float = 0.9\n",
    ") -> List[str]:\n",
    "    model.eval()\n",
    "    generations = []\n",
    "    for prompt in prompts:\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "        generated = model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_k=50,\n",
    "            top_p=top_p\n",
    "        )\n",
    "        text = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "        generations.append(text)\n",
    "        logger.info(f\"Prompt: {prompt}\\nGenerated: {text}\")\n",
    "    return generations\n",
    "\n",
    "# ====================== Data Preprocessing ======================\n",
    "def preprocess_tinystories(tokenizer: AutoTokenizer, max_samples: int = 5000, max_length: int = 64) -> List[str]:\n",
    "    logger.info(\"Loading TinyStories dataset...\")\n",
    "    dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train\", streaming=True)\n",
    "\n",
    "    texts = []\n",
    "    for i, sample in tqdm(enumerate(dataset), total=max_samples, desc=\"Processing TinyStories\", unit=\"story\"):\n",
    "        if i >= max_samples:\n",
    "            break\n",
    "        text = sample['text'].strip()\n",
    "        tokens = tokenizer(text, truncation=True, max_length=max_length, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "        truncated_text = tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "        if truncated_text:\n",
    "            texts.append(truncated_text)\n",
    "\n",
    "    logger.info(f\"Collected {len(texts)} text samples from TinyStories\")\n",
    "    return texts\n",
    "\n",
    "# ====================== Main Execution ======================\n",
    "def main():\n",
    "    global OUTPUT_DIR\n",
    "    # Config\n",
    "    MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "    HIDDEN_DIM = 512\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    MAX_SAMPLES = 5000\n",
    "    EPOCHS = 3\n",
    "    BATCH_SIZE = 8\n",
    "    ACCUM_STEPS = 8\n",
    "    OUTPUT_DIR = \"/content/pnn_llama_distillation\"\n",
    "\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    # Load tokenizer and add special tokens\n",
    "    logger.info(\"Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    if tokenizer.mask_token is None:\n",
    "        tokenizer.add_special_tokens({'mask_token': '[MASK]'})\n",
    "    if tokenizer.eos_token_id is None:\n",
    "        logger.warning(\"eos_token_id is None, setting to default\")\n",
    "        tokenizer.eos_token_id = tokenizer.convert_tokens_to_ids(tokenizer.eos_token)\n",
    "    logger.info(f\"Tokenizer vocab size: {len(tokenizer)}, eos_token_id: {tokenizer.eos_token_id}\")\n",
    "\n",
    "    # Load model and resize embeddings\n",
    "    logger.info(\"Loading model...\")\n",
    "    quant_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True,\n",
    "        bnb_8bit_compute_dtype=torch.float16,\n",
    "        bnb_8bit_use_double_quant=True\n",
    "    )\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=quant_config,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    base_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    # Initialize PNN\n",
    "    model = PNNWithLLaMA(\n",
    "        base_model=base_model,\n",
    "        hidden_dim=HIDDEN_DIM,\n",
    "        vocab_size=len(tokenizer),\n",
    "        tokenizer=tokenizer\n",
    "    ).to(DEVICE)\n",
    "    assert model.vocab_size == len(tokenizer), f\"Vocab size mismatch: model {model.vocab_size}, tokenizer {len(tokenizer)}\"\n",
    "\n",
    "    # Load and preprocess TinyStories dataset\n",
    "    texts = preprocess_tinystories(tokenizer, max_samples=MAX_SAMPLES)\n",
    "\n",
    "    # Split dataset\n",
    "    train_texts = texts[:int(0.9 * len(texts))]\n",
    "    eval_texts = texts[int(0.9 * len(texts)):]\n",
    "\n",
    "    logger.info(\"Training with SSL (CLM) and distillation...\")\n",
    "    train_ssl_task(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        texts=train_texts,\n",
    "        column_idx=0,\n",
    "        device=DEVICE,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        max_length=128,\n",
    "        accum_steps=ACCUM_STEPS,\n",
    "        distill_hidden_weight=0.3,\n",
    "        distill_logits_weight=0.3\n",
    "    )\n",
    "\n",
    "    # Save final model and tokenizer\n",
    "    final_model_path = os.path.join(OUTPUT_DIR, \"model.pt\")\n",
    "    torch.save(model.state_dict(), final_model_path)\n",
    "    logger.info(f\"Saved final model: {final_model_path}\")\n",
    "\n",
    "    tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "    logger.info(f\"Saved tokenizer: {OUTPUT_DIR}\")\n",
    "\n",
    "    # Evaluate perplexity\n",
    "    perplexity = evaluate_perplexity(model, tokenizer, eval_texts, DEVICE, batch_size=BATCH_SIZE)\n",
    "    logger.info(f\"Perplexity on evaluation set: {perplexity:.4f}\")\n",
    "\n",
    "    # Evaluate generation\n",
    "    prompts = [\n",
    "        \"Once upon a time, in a forest far away,\",\n",
    "        \"There was a little robot who loved to explore.\",\n",
    "        \"In a quiet village, a child found a magical book.\"\n",
    "    ]\n",
    "    generations = evaluate_generation(model, tokenizer, prompts, DEVICE, max_length=100)\n",
    "    logger.info(\"Generated samples for review:\")\n",
    "    for prompt, gen in zip(prompts, generations):\n",
    "        logger.info(f\"Prompt: {prompt}\\nGenerated: {gen}\\n\")\n",
    "\n",
    "    if DEVICE.type == 'cuda':\n",
    "        logger.info(f\"Peak GPU Memory: {torch.cuda.max_memory_allocated()/1e9:.2f} GB\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
