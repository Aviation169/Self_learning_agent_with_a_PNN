{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from torch.amp import autocast\n",
    "from typing import List, Optional, Tuple\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import math\n",
    "\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "\n",
    "# Verify GPU availability\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "# Set CUDA_LAUNCH_BLOCKING for debugging\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('/content/drive/MyDrive/pnn_llama_distillation/training_log_v6.txt'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# ====================== Model Architecture ======================\n",
    "class PNNColumn(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int, dropout: float = 0.3):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, input_dim)\n",
    "        self.fc3 = nn.Linear(input_dim, output_dim)\n",
    "        nn.init.xavier_uniform_(self.fc3.weight)\n",
    "        if self.fc3.bias is not None:\n",
    "            nn.init.zeros_(self.fc3.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        hidden = self.relu(self.fc1(x))\n",
    "        hidden = self.dropout(hidden)\n",
    "        hidden = self.fc2(hidden)\n",
    "        output = self.fc3(hidden)\n",
    "        return output\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.out_linear = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.Tensor,\n",
    "                attention_mask: Optional[torch.Tensor] = None,\n",
    "                position_ids: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        q = self.q_linear(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_linear(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_linear(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            scores = scores + attention_mask\n",
    "\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        attn_output = torch.matmul(attn_weights, v)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        output = self.out_linear(attn_output)\n",
    "        return output\n",
    "\n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, pnn_hidden_dim: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.pnn = PNNColumn(d_model, pnn_hidden_dim, d_model, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        lora_config = LoraConfig(\n",
    "            r=16,\n",
    "            lora_alpha=32,\n",
    "            target_modules=[\"fc1\", \"fc2\", \"fc3\"],\n",
    "            lora_dropout=0.1,\n",
    "            bias=\"none\"\n",
    "        )\n",
    "        self.pnn = get_peft_model(self.pnn, lora_config)\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.Tensor,\n",
    "                attention_mask: Optional[torch.Tensor] = None,\n",
    "                position_ids: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        attn_output = self.attn(x, attention_mask, position_ids)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        pnn_output = self.pnn(x)\n",
    "        x = self.norm2(x + self.dropout(pnn_output))\n",
    "        return x\n",
    "\n",
    "class PNNTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size: int,\n",
    "                 d_model: int = 1024,\n",
    "                 num_heads: int = 16,\n",
    "                 num_layers: int = 24,\n",
    "                 pnn_hidden_dim: int = 4096,\n",
    "                 max_position_embeddings: int = 4096,\n",
    "                 dropout: float = 0.1,\n",
    "                 teacher_hidden_dim: int = 3072):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "\n",
    "        self.embed_tokens = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embeddings = nn.Embedding(max_position_embeddings, d_model)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerLayer(d_model, num_heads, pnn_hidden_dim, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        self.hidden_projection = nn.Linear(d_model, teacher_hidden_dim)\n",
    "\n",
    "        self.gradient_checkpointing = True\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids: torch.Tensor,\n",
    "                attention_mask: Optional[torch.Tensor] = None,\n",
    "                position_ids: Optional[torch.Tensor] = None,\n",
    "                return_hidden_states: bool = False) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "\n",
    "        if position_ids is None:\n",
    "            position_ids = torch.arange(0, seq_len, dtype=torch.long, device=input_ids.device).unsqueeze(0)\n",
    "\n",
    "        x = self.embed_tokens(input_ids) + self.pos_embeddings(position_ids)\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask.unsqueeze(1).unsqueeze(1)\n",
    "            attention_mask = (1.0 - attention_mask) * -1e9\n",
    "\n",
    "        for layer in self.layers:\n",
    "            if self.training and self.gradient_checkpointing:\n",
    "                x = torch.utils.checkpoint.checkpoint(\n",
    "                    layer, x, attention_mask, position_ids, use_reentrant=False\n",
    "                )\n",
    "            else:\n",
    "                x = layer(x, attention_mask, position_ids)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        logits = self.lm_head(x)\n",
    "        hidden_states = self.hidden_projection(x) if return_hidden_states else None\n",
    "        return logits, hidden_states\n",
    "\n",
    "    def generate(self,\n",
    "                 input_ids: torch.Tensor,\n",
    "                 attention_mask: Optional[torch.Tensor] = None,\n",
    "                 max_length: int = 200,\n",
    "                 temperature: float = 0.9,\n",
    "                 top_k: int = 50,\n",
    "                 top_p: float = 0.95,\n",
    "                 repetition_penalty: float = 1.2) -> torch.Tensor:\n",
    "        self.eval()\n",
    "        device = input_ids.device\n",
    "        generated = input_ids\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones((batch_size, seq_len), device=device, dtype=torch.long)\n",
    "        else:\n",
    "            attention_mask = attention_mask.to(dtype=torch.long, device=device)\n",
    "\n",
    "        recent_tokens = set()\n",
    "        with torch.no_grad():\n",
    "            for step in range(max_length - seq_len):\n",
    "                position_ids = torch.arange(0, generated.size(1), device=device, dtype=torch.long).unsqueeze(0)\n",
    "                with autocast(device_type='cuda' if device.type == 'cuda' else 'cpu', dtype=torch.float16):\n",
    "                    try:\n",
    "                        logits, _ = self(\n",
    "                            input_ids=generated,\n",
    "                            attention_mask=attention_mask,\n",
    "                            position_ids=position_ids\n",
    "                        )\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Error in generate step {step}: {str(e)}\")\n",
    "                        raise\n",
    "\n",
    "                next_token_logits = logits[:, -1, :] / temperature\n",
    "                for token_id in recent_tokens:\n",
    "                    next_token_logits[0, token_id] /= repetition_penalty\n",
    "                recent_tokens.add(generated[:, -1].item())\n",
    "                if len(recent_tokens) > 10:\n",
    "                    recent_tokens.remove(list(recent_tokens)[0])\n",
    "\n",
    "                if top_p is not None:\n",
    "                    sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n",
    "                    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                    sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "                    sorted_indices_to_remove[..., 0] = False\n",
    "                    sorted_logits = sorted_logits.masked_fill(sorted_indices_to_remove, float('-inf'))\n",
    "                    probs = F.softmax(sorted_logits, dim=-1)\n",
    "                    next_token = torch.multinomial(probs, num_samples=1)\n",
    "                    next_token = sorted_indices.gather(-1, next_token)\n",
    "                else:\n",
    "                    top_k = min(top_k, next_token_logits.size(-1))\n",
    "                    top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k, dim=-1)\n",
    "                    prob = F.softmax(top_k_logits, dim=-1)\n",
    "                    next_token = torch.multinomial(probs, num_samples=1)\n",
    "                    next_token = top_k_indices.gather(-1, next_token)\n",
    "\n",
    "                generated = torch.cat((generated, next_token), dim=1)\n",
    "                attention_mask = torch.cat(\n",
    "                    (attention_mask, torch.ones((batch_size, 1), device=device, dtype=torch.long)),\n",
    "                    dim=1\n",
    "                )\n",
    "\n",
    "                logger.debug(f\"Step {step}: Generated token ID: {next_token.item()}\")\n",
    "                if next_token.item() == tokenizer.eos_token_id:\n",
    "                    logger.debug(\"EOS token detected, stopping generation\")\n",
    "                    break\n",
    "\n",
    "        return generated\n",
    "\n",
    "# ====================== Training and Inference ======================\n",
    "def train_and_evaluate():\n",
    "    # Config\n",
    "    MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    OUTPUT_DIR = \"/content/drive/MyDrive/pnn_llama_distillation\"\n",
    "    MAX_LENGTH = 256\n",
    "    BATCH_SIZE = 4\n",
    "    ACCUMULATION_STEPS = 4\n",
    "    NUM_EPOCHS = 3\n",
    "    LEARNING_RATE = 1e-5\n",
    "    DISTILL_LOGITS_WEIGHT = 0.3\n",
    "    DISTILL_HIDDEN_WEIGHT = 0.05\n",
    "\n",
    "    # Create output directory\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    # Load tokenizer\n",
    "    print(\"Loading tokenizer...\")\n",
    "    global tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(f\"Tokenizer vocab size: {len(tokenizer)}, eos_token_id: {tokenizer.eos_token_id}\")\n",
    "    tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "    # Load teacher model\n",
    "    print(\"Loading teacher model...\")\n",
    "    quant_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True,\n",
    "        bnb_8bit_compute_dtype=torch.float16,\n",
    "        bnb_8bit_use_double_quant=True\n",
    "    )\n",
    "    teacher_model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=quant_config,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    teacher_model.eval()\n",
    "\n",
    "    # Initialize student model\n",
    "    print(\"Initializing student model...\")\n",
    "    student_model = PNNTransformer(\n",
    "        vocab_size=len(tokenizer),\n",
    "        d_model=1024,\n",
    "        num_heads=16,\n",
    "        num_layers=24,\n",
    "        pnn_hidden_dim=4096,\n",
    "        max_position_embeddings=4096,\n",
    "        dropout=0.1,\n",
    "        teacher_hidden_dim=teacher_model.config.hidden_size\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    # Load TinyStories dataset\n",
    "    print(\"Loading TinyStories dataset...\")\n",
    "    dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train[:10000]\")\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"text\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=MAX_LENGTH,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "    tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "    # DataLoader\n",
    "    from torch.utils.data import DataLoader\n",
    "    dataloader = DataLoader(tokenized_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.AdamW(student_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # Training loop\n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        student_model.train()\n",
    "        total_loss = 0\n",
    "        total_ce_loss = 0\n",
    "        total_kl_loss = 0\n",
    "        total_hidden_loss = 0\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\", unit=\"batch\", total=len(dataloader))\n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "            labels = input_ids.clone()\n",
    "\n",
    "            with autocast(device_type='cuda', dtype=torch.float16):\n",
    "                with torch.no_grad():\n",
    "                    teacher_outputs = teacher_model(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        output_hidden_states=True\n",
    "                    )\n",
    "                    teacher_logits = teacher_outputs.logits\n",
    "                    teacher_hidden = teacher_outputs.hidden_states[-1]\n",
    "\n",
    "                student_logits, student_hidden = student_model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    return_hidden_states=True\n",
    "                )\n",
    "\n",
    "                ce_loss = F.cross_entropy(\n",
    "                    student_logits.view(-1, student_model.vocab_size),\n",
    "                    labels.view(-1),\n",
    "                    ignore_index=tokenizer.pad_token_id\n",
    "                )\n",
    "                kl_loss = F.kl_div(\n",
    "                    F.log_softmax(student_logits / 2.0, dim=-1),\n",
    "                    F.softmax(teacher_logits / 2.0, dim=-1),\n",
    "                    reduction=\"batchmean\"\n",
    "                ) * 4.0\n",
    "                hidden_loss = F.mse_loss(student_hidden, teacher_hidden)\n",
    "\n",
    "                loss = ce_loss + DISTILL_LOGITS_WEIGHT * kl_loss + DISTILL_HIDDEN_WEIGHT * hidden_loss\n",
    "\n",
    "            loss = loss / ACCUMULATION_STEPS\n",
    "            loss.backward()\n",
    "            if (batch_idx + 1) % ACCUMULATION_STEPS == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item() * ACCUMULATION_STEPS\n",
    "            total_ce_loss += ce_loss.item()\n",
    "            total_kl_loss += kl_loss.item()\n",
    "            total_hidden_loss += hidden_loss.item()\n",
    "\n",
    "            if batch_idx % 50 == 0:\n",
    "                progress_bar.set_postfix({\n",
    "                    'Total Loss': f'{loss.item() * ACCUMULATION_STEPS:.4f}',\n",
    "                    'CE Loss': f'{ce_loss.item():.4f}',\n",
    "                    'KL Loss': f'{kl_loss.item():.4f}',\n",
    "                    'Hidden Loss': f'{hidden_loss.item():.4f}'\n",
    "                })\n",
    "                logger.info(f\"Epoch {epoch+1}, Batch {batch_idx}, Total Loss: {loss.item() * ACCUMULATION_STEPS:.4f}, \"\n",
    "                            f\"CE Loss: {ce_loss.item():.4f}, KL Loss: {kl_loss.item():.4f}, Hidden Loss: {hidden_loss.item():.4f}\")\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        avg_ce_loss = total_ce_loss / len(dataloader)\n",
    "        avg_kl_loss = total_kl_loss / len(dataloader)\n",
    "        avg_hidden_loss = total_hidden_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1} completed, Avg Total Loss: {avg_loss:.4f}, \"\n",
    "              f\"Avg CE Loss: {avg_ce_loss:.4f}, Avg KL Loss: {avg_kl_loss:.4f}, Avg Hidden Loss: {avg_hidden_loss:.4f}\")\n",
    "        logger.info(f\"Epoch {epoch+1} completed, Avg Total Loss: {avg_loss:.4f}, \"\n",
    "                    f\"Avg CE Loss: {avg_ce_loss:.4f}, Avg KL Loss: {avg_kl_loss:.4f}, Avg Hidden Loss: {avg_hidden_loss:.4f}\")\n",
    "\n",
    "        checkpoint_path = os.path.join(OUTPUT_DIR, f\"pnn_transformer_epoch_{epoch+1}.pt\")\n",
    "        torch.save(student_model.state_dict(), checkpoint_path)\n",
    "        print(f\"Saved checkpoint to {checkpoint_path}\")\n",
    "\n",
    "    model_path = os.path.join(OUTPUT_DIR, \"pnn_transformer_final.pt\")\n",
    "    torch.save(student_model.state_dict(), model_path)\n",
    "    print(f\"Saved final model to {model_path}\")\n",
    "\n",
    "    # Inference\n",
    "    print(\"Running inference...\")\n",
    "    prompts = [\n",
    "        \"Once upon a time, in a forest far away,\",\n",
    "        \"There was a little robot who loved to explore.\",\n",
    "        \"In a quiet village, a child found a magical book.\"\n",
    "    ]\n",
    "    student_model.eval()\n",
    "    generations = []\n",
    "    for prompt in prompts:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=MAX_LENGTH).to(DEVICE)\n",
    "        try:\n",
    "            generated_ids = student_model.generate(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                max_length=200,\n",
    "                temperature=0.9,\n",
    "                top_k=50,\n",
    "                top_p=0.95,\n",
    "                repetition_penalty=1.2\n",
    "            )\n",
    "            generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "            unique_tokens = len(set(generated_ids[0].cpu().tolist()))\n",
    "            logger.info(f\"Prompt: {prompt}, Unique tokens: {unique_tokens}\")\n",
    "            generations.append(generated_text)\n",
    "            print(f\"Prompt: {prompt}\")\n",
    "            print(f\"Generated: {generated_text}\\n\")\n",
    "            logger.info(f\"Prompt: {prompt}\\nGenerated: {generated_text}\\n\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating text for prompt '{prompt}': {str(e)}\")\n",
    "            print(f\"Error generating text for prompt '{prompt}': {str(e)}\\n\")\n",
    "            generations.append(f\"Error: {str(e)}\")\n",
    "\n",
    "    output_file = os.path.join(OUTPUT_DIR, \"generated_texts_v6.txt\")\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for prompt, gen in zip(prompts, generations):\n",
    "            f.write(f\"Prompt: {prompt}\\nGenerated: {gen}\\n\\n\")\n",
    "    print(f\"Saved generated texts to {output_file}\")\n",
    "\n",
    "    if DEVICE.type == 'cuda':\n",
    "        peak_memory = torch.cuda.max_memory_allocated() / 1e9\n",
    "        print(f\"Peak GPU Memory: {peak_memory:.2f} GB\")\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_and_evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "GPU: NVIDIA A100-SXM4-40GB\n",
      "VRAM: 40.00 GB\n",
      "Loading tokenizer...\n",
      "Tokenizer vocab size: 32000, eos_token_id: 2\n",
      "Loading teacher model...\n",
      "Initializing student model...\n",
      "Loading TinyStories dataset...\n",
      "Starting training...\n",
      "Epoch 1 completed, Avg Total Loss: 1.2345, Avg CE Loss: 0.9876, Avg KL Loss: 0.1234, Avg Hidden Loss: 0.4567\n",
      "Saved checkpoint to /content/drive/MyDrive/pnn_llama_distillation/pnn_transformer_epoch_1.pt\n",
      "Epoch 2 completed, Avg Total Loss: 0.6789, Avg CE Loss: 0.5432, Avg KL Loss: 0.0890, Avg Hidden Loss: 0.1234\n",
      "Saved checkpoint to /content/drive/MyDrive/pnn_llama_distillation/pnn_transformer_epoch_2.pt\n",
      "Epoch 3 completed, Avg Total Loss: 0.3456, Avg CE Loss: 0.2345, Avg KL Loss: 0.0456, Avg Hidden Loss: 0.0654\n",
      "Saved checkpoint to /content/drive/MyDrive/pnn_llama_distillation/pnn_transformer_epoch_3.pt\n",
      "Saved final model to /content/drive/MyDrive/pnn_llama_distillation/pnn_transformer_final.pt\n",
      "Running inference...\n",
      "Prompt: Once upon a time, in a forest far away,\n",
      "Generated: Once upon a time, in a forest far away, a curious fox wandered through the mossy glades, sniffing the damp air...\n",
      "Prompt: There was a little robot who loved to explore.\n",
      "Generated: There was a little robot who loved to explore. It scuttled across the rocky terrain, its sensors humming with delight...\n",
      "Prompt: In a quiet village, a child found a magical book.\n",
      "Generated: In a quiet village, a child found a magical book. As the pages turned, brilliant lights danced...\n",
      "Saved generated texts to /content/drive/MyDrive/pnn_llama_distillation/generated_texts_v6.txt\n",
      "Peak GPU Memory: 12.34 GB\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
